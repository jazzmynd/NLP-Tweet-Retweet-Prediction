{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da3e8a",
   "metadata": {
    "id": "96da3e8a"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from google.colab import files\n",
    "import bcolz\n",
    "import pickle\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ghIfS47AVBd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ghIfS47AVBd1",
    "outputId": "3f2e973b-0b45-4c49-e638-5e58d3b87ec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bcolz\n",
      "  Downloading bcolz-1.2.1.tar.gz (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 8.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from bcolz) (1.21.6)\n",
      "Building wheels for collected packages: bcolz\n",
      "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for bcolz: filename=bcolz-1.2.1-cp37-cp37m-linux_x86_64.whl size=2653175 sha256=062f702e9b611aad11d1f6ad79eeab4af7d6d323917fe125ccb6fb166224a33e\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/35/ca/9d914de345914e2446ea285170329f771b8abba2a00f7650bd\n",
      "Successfully built bcolz\n",
      "Installing collected packages: bcolz\n",
      "Successfully installed bcolz-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bcolz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ac567",
   "metadata": {
    "id": "7a3ac567"
   },
   "source": [
    "# Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adKAoZQWWzm7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adKAoZQWWzm7",
    "outputId": "e6a5fb51-06fd-4142-d42b-2410573259a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tczYHi0oWuhZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tczYHi0oWuhZ",
    "outputId": "3bdf0b94-ce1b-4a60-fdce-4fd63fa75f3f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "glove_path = '/content/drive/MyDrive/505/project/CS505-final-project-main'\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=f'{glove_path}/6B.100.dat', mode='w')\n",
    "\n",
    "with open(f'{glove_path}/glove.6B.100d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "    \n",
    "vectors = bcolz.carray(vectors[1:].reshape((400000, 100)), rootdir=f'{glove_path}/6B.100.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(f'{glove_path}/6B.100_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(f'{glove_path}/6B.100_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd4274",
   "metadata": {
    "id": "96dd4274"
   },
   "outputs": [],
   "source": [
    "vectors = bcolz.open(f'{glove_path}/6B.100.dat')[:]\n",
    "words = pickle.load(open(f'{glove_path}/6B.100_words.pkl', 'rb'))\n",
    "words += ['<UNK>', '<s>', '</s>', 'PAD']\n",
    "vocab_list_glove = set(words)\n",
    "new_vecs = np.random.normal(loc=0.0, scale=.6, size=(4,100) )\n",
    "vectors = np.vstack((vectors, new_vecs))\n",
    "word2idx = pickle.load(open(f'{glove_path}/6B.100_idx.pkl', 'rb'))\n",
    "word2idx['<UNK>'] = 400000\n",
    "word2idx['<s>'] = 400001\n",
    "word2idx['</s>'] = 400002\n",
    "word2idx['PAD'] = 400003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fdvPMKB8N_",
   "metadata": {
    "id": "57fdvPMKB8N_"
   },
   "source": [
    "## Tweets pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "YRMqlKGAny0z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRMqlKGAny0z",
    "outputId": "0ec26531-2152-41c1-8eb6-3bdd5b48430a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96562\n"
     ]
    }
   ],
   "source": [
    "path = '/content/drive/MyDrive/505/project/'\n",
    "\n",
    "\n",
    "def read_json(filename: str):\n",
    "    with open(filename, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Extract tweets and retweet numbers\n",
    "train = read_json(f'{path}/train.json')\n",
    "content = [item['text'] for item in train]\n",
    "retweet_num = [item['retweet_count'] for item in train]\n",
    "print(len(retweet_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "zEg9DfW93BO2",
   "metadata": {
    "id": "zEg9DfW93BO2"
   },
   "outputs": [],
   "source": [
    "# Clean tweets\n",
    "def pre_process(data):\n",
    "  clean_data = []\n",
    "  for line in data:\n",
    "    line = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"url\", line) \n",
    "    line = re.sub(\"[^a-z@' ]\", \"\", line.lower()) \n",
    "    doc = nlp(line)\n",
    "    temp = []\n",
    "    for token in doc: \n",
    "      if token.text == 'url':\n",
    "        temp.append('<url>')\n",
    "      else:\n",
    "        temp.append(token.text)\n",
    "    \n",
    "    temp_2 = \" \".join(temp)  \n",
    "    text = re.sub(r\"@\\w+\", \"<user>\", temp_2)\n",
    "    clean_data.append(text)\n",
    "\n",
    "  return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6qA5BXN13VBr",
   "metadata": {
    "id": "6qA5BXN13VBr"
   },
   "outputs": [],
   "source": [
    "# Save pre-process tweets\n",
    "tweets = pre_process(content)\n",
    "file = open(f'{path}/train_clean_text.txt', \"w\")\n",
    "for item in tweets:\n",
    "    file.write(item + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OhmJY9Gf3vWE",
   "metadata": {
    "id": "OhmJY9Gf3vWE"
   },
   "outputs": [],
   "source": [
    "# Loading processed tweets\n",
    "tweets = pd.read_csv(f'{path}/train_clean_text.txt', header = None)\n",
    "tweets.columns = ['tweet']\n",
    "tweets = tweets.tweet.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "T_aP9t3CuHEJ",
   "metadata": {
    "id": "T_aP9t3CuHEJ"
   },
   "outputs": [],
   "source": [
    "# Not using slicing window padding\n",
    "def padding(data, seq_len):\n",
    "  sequences = []\n",
    "  for line in data:\n",
    "    n_token = len(line.split())\n",
    "    \n",
    "    if n_token > seq_len:\n",
    "      seq = f\"{'<s>'} {line} {'</s>'}\".split()[:seq_len] \n",
    "      sequences.append(\" \".join(seq))\n",
    "\n",
    "    else:\n",
    "      seq = f\"{'<s>'} {line} {'</s>'}\".split()\n",
    "      for i in range(seq_len - n_token):\n",
    "          seq.append('PAD')\n",
    "      sequences.append(\" \".join(seq))\n",
    "  return sequences\n",
    "\n",
    "\n",
    "# Building vocabulary\n",
    "def word_count(data):\n",
    "  words_counter = Counter()\n",
    "  for line in data:\n",
    "    words =  line.split()\n",
    "    for w in words:\n",
    "      words_counter.update([w])\n",
    "  \n",
    "  words_counter_clean = {k:v for k,v in words_counter.items() if v > 1} # Removing the words that only appear once\n",
    "  sorted_words = sorted(words_counter_clean, key = words_counter_clean.get, reverse = True) # Sorting the words frequency in desc order\n",
    "  sorted_words = ['UNK'] + sorted_words # 'PAD', '<s>', '</s>' already in vocab\n",
    "\n",
    "  return words_counter, words_counter_clean, sorted_words\n",
    "\n",
    "# replace the words that only appear once with UNKNOWN\n",
    "def generate_sentence(data):\n",
    "  sequences = []\n",
    "  for line in data:\n",
    "    temp = []\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "      if word in sorted_words:\n",
    "        temp.append(word)\n",
    "      else:\n",
    "        temp.append('UNK')\n",
    "    sequences.append(\" \".join(temp))\n",
    "  return sequences\n",
    "\n",
    "\n",
    "tweets_pad = padding(tweets)\n",
    "words_counter, words_counter_clean, sorted_words = word_count(tweets)\n",
    "tweets_final = generate_sentence(tweets_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ii7TYo3rL1-",
   "metadata": {
    "id": "6ii7TYo3rL1-"
   },
   "outputs": [],
   "source": [
    "# Using glove weights\n",
    "glove = {w: vectors[word2idx[w]] for w in words}\n",
    "matrix_len = len(sorted_words)\n",
    "weights_matrix = np.zeros((matrix_len, 100))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(sorted_words):\n",
    "  try: \n",
    "    weights_matrix[i] = glove[word] # if alr in the vocab, load its pre-trained word vector.\n",
    "    words_found += 1\n",
    "  except KeyError:\n",
    "    weights_matrix[i] = np.random.normal(scale=0.6, size=(100, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o54nmNt2w4sd",
   "metadata": {
    "id": "o54nmNt2w4sd"
   },
   "outputs": [],
   "source": [
    "# Using tweets vocabulary\n",
    "# Dictionaries to store the word to index mappings and vice versa\n",
    "word2idx = {o:i for i,o in enumerate(sorted_words)}\n",
    "idx2word = {i:o for i,o in enumerate(sorted_words)}\n",
    "\n",
    "\n",
    "# convert text sequences to integer sequences\n",
    "tweet_int = []\n",
    "for data in tweets_final:\n",
    "    temp = [word2idx[w] for w in data.split()]\n",
    "    tweet_int.append(temp)\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "tweet_int = np.array(tweet_int)\n",
    "retweet_int = np.array(retweet_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b8d40",
   "metadata": {
    "id": "5e5b8d40"
   },
   "source": [
    "# Neural Net\n",
    "\n",
    "### Retweet Network: Takes in a tweet as input, can use embedded version, and can any combination of bidirectional, LSTM, GRU, concatenates it with metadata vector, and uses a feedforward neural net with 1 hidden layer to perform a regression prediction on the retweet count. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c224d1",
   "metadata": {},
   "source": [
    "#### Parameter custom_embeddings is either a tuple: (weight_matrix , none_trainable), or None.\n",
    "#### none_trainable is either True or False or Nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8369491b",
   "metadata": {
    "id": "8369491b"
   },
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': torch.from_numpy(weights_matrix)})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class RetweetNet(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_state_sizes, meta_data_len, output_size, embedding_dim, hidden_dim, \n",
    "                 n_layers, drop_prob=0.5, custom_embeddings = None, bidirectional = False, GRU = False):\n",
    "        super().__init__()\n",
    "        self.GRU_val = GRU\n",
    "        self.bidirectional = bidirectional\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        if custom_embeddings is None: \n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        else: \n",
    "            assert len(custom_embeddings) == 2 and isinstance(custom_embeddings, tuple), \"custom embeddings must be of form: (weight_matrix, non_trainable)\"\n",
    "            weights_matrix, non_trainable = custom_embeddings\n",
    "            self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, non_trainable)\n",
    "        \n",
    "        if GRU == False: \n",
    "            self.Gate = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                               dropout=drop_prob, batch_first=True, bidirectional = bidirectional)\n",
    "        else: \n",
    "            self.Gate = nn.GRU(embedding_dim, hidden_dim, n_layers, \n",
    "                              dropout=drop_prob, batch_first=True, bidirectional = bidirectional)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_state_sizes[0])\n",
    "        self.relu = nn.RELU()\n",
    "        \n",
    "        #hidden_state_sizes[0] is the size of the output of lstm \n",
    "        self.fc2 = nn.Linear(hidden_state_sizes[0] + meta_data_len, hidden_state_sizes[1])\n",
    "        \n",
    "        #hidden_state_sizes[1] is the size of the first and only hidden layer\n",
    "        self.fc3 = nn.Linear(hidden_state_sizes[1], 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x, meta_data, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        gru_out, hidden = self.Gate(embeds, hidden)\n",
    "        gru_out = gru_out.contiguous().view(-1, self.hidden_dim)\n",
    "    \n",
    "        out = self.dropout(gru_out)\n",
    "        out = self.fc1(out)\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1] \n",
    "        meta_data = meta_data.view(batch_size, -1)\n",
    "        \n",
    "        # combine hidden state and meta_data\n",
    "        out = torch.cat((out, meta_data), dim = 1) #meta_data is of shape (batch_size, -1)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        # applying dropout before relu since relu already sets some neurons to 0\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        n = 1\n",
    "        if self.bidirectional == True: \n",
    "            n = 2\n",
    "        if self.GRU_val == False:\n",
    "            return (weight.new(self.n_layers * n, batch_size, self.hidden_dim).zero_().to('cuda'),\n",
    "                      weight.new(self.n_layers * n, batch_size, self.hidden_dim).zero_().to('cuda'))\n",
    "        return  weight.new(self.n_layers * n, batch_size, self.hidden_dim).zero_().to('cuda')\n",
    "\n",
    "\n",
    "def train_retweet_predictor(model, epochs = 2,print_every = 1000, clip = 5, valid_loss_min = np.Inf, \n",
    "                   lr=0.005, batch_size = 400, device = 'cuda', GRU = False, weight_decay = 1e-5): \n",
    "    counter = 0\n",
    "    model.train()\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # weight decay is the l2 regularization penalty \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        h = model.init_hidden(batch_size)\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        if GRU == False: \n",
    "            h = tuple([each.data for each in h])\n",
    "        else:\n",
    "            h = h.data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "            \"Step: {}...\".format(counter),\n",
    "            \"Loss: {:.6f}...\".format(loss.item()))\n",
    "          \n",
    "def error_retweet_predictor(model, batch_size = 359, device = 'cuda', GRU = False): \n",
    "    test_losses = []\n",
    "    num_correct = []\n",
    "    model.cuda()\n",
    "\n",
    "    h = model.init_hidden(batch_size)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.eval()\n",
    "    for inputs, labels in test_loader:\n",
    "        if GRU == True: \n",
    "            h = h.data\n",
    "        else: \n",
    "            h = tuple([each.data for each in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output, h = model(inputs, h)\n",
    "        test_loss = criterion(output.squeeze(), labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "        pred = torch.round(output.squeeze())\n",
    "        \n",
    "        errors = torch.sum(torch.square(pred - labels.float().view_as(pred)), \n",
    "                           axis= 1)/(predicted_x.size()[0]\n",
    "        num_correct.append(np.squeeze(errors.cpu().numpy()))     \n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "    print(\"Test accuracy: {:.3f}%\".format(np.mean(num_correct)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Retweet Model 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
